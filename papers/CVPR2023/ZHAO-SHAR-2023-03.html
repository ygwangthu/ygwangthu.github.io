<!DOCTYPE html>
<!-- saved from url=(0055)https://www.yangangwang.com/papers/ZZM-SCR-2023-03.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination</title>
	
	<meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1">
    <!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="../../assets/css/main.css" />
    
    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.10/jquery.lazy.min.js"></script>
</head>

<body class="">
    <div id="wrapper">
        <div id="main" class="panel" style="margin-top:2em;line-height:1.3em">
            <article id="news" class="panel">
                <!-- head -->
                <div style="font-size:0.75em;text-align:center;padding-top:0.5em">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</div>
                <div style="font-size:1.0em;font-weight:bold;padding:1em 0.5em;text-align:center">Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination</div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;"><a target="_blank" href="https://scholar.google.com/citations?user=XtBkmC8AAAAJ&amp;hl=en" style="color:#EB7500">Zimeng Zhao</a>, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="https://binghui-z.github.io/" style="color:#EB7500">Binghui Zuo</a>, </div>&nbsp;&nbsp;
					<div style="display:inline-block;">Zhiyu Long, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="https://www.yangangwang.com/" style="color:#EB7500">Yangang Wang</a></div>
                </div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;"><a target="_blank" href="http://www.seu.edu.cn/english/main.htm" style="font-weight:bold">Southeast University</a></div>
                </div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;;margin-bottom:1em">

                <!-- teaser -->
                <div style="text-align:center;"><img class="lazy" width="100%" src="./pipeline.jpg" style=""></div>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em"><div style="font-weight:bold;display:inline-block">Hand appearance recovery pipeline.</div> (a)~(c) Learning modules to disentangle bare hand structure map. Then a generative translator learns to wrap the structure map with the appearance in original image through DAD scheme. </div>

                <!-- abstract -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em;">Abstract</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="text-align:left;font-size:0.75em;line-height:1.4em">Enormous hand images with reliable annotations are collected through marker-based MoCap. Unfortunately, degradations caused by markers limit their application in hand appearance reconstruction. A clear appearance recovery insight is an image-to-image translation trained with unpaired data. However, most frameworks fail because there exists structure inconsistency from a degraded hand to a bare one. The core of our approach is to <b>first disentangle the bare hand structure from those degraded images and then wrap the appearance to this structure with a dual adversarial discrimination (DAD) scheme.</b> Both modules take full advantage of the semi-supervised learning paradigm: The structure disentanglement benefits from the modeling ability of ViT, and the translator is enhanced by the dual discrimination on both translation processes and translation results. Comprehensive evaluations have been conducted to prove that our framework can robustly recover photo-realistic hand appearance from diverse marker-contained and even object-occluded datasets. It provides a novel avenue to acquire bare hand appearance data for other downstream learning problems. </div>
                
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em;">Semi-supervised Paradigm</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="text-align:center;"><img class="lazy" width="100%" src="./system.jpg" style=""></div>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em">
                <b>DAD scheme and Translator architecture.</b> (a) DAD introduces both result discriminator and process discriminator together to judge a multimodal translation process. (b) Our translator takes the estimated structure map and original image together as the inputs and separately extracts their multi-level features with a shared CNN backbone.
                </div>

                <!-- results -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Results</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">    
                
                <div style="text-align:center"><img class="lazy" width="90%" src="./totalres.jpg"></div>
                <div style="font-size:0.75em;text-align:center;line-height:1.4em;">
                    <b>Qualitative recovery results</b> from marker-contained data and object-occluded data.
                </div>
                
                <div style="text-align:center"><img class="lazy" width="90%" src="./compareNormal.jpg"></div>
                <div style="font-size:0.75em;text-align:center;line-height:1.4em;">
                    <b>Comparisons on Structure disentanglements.</b> (Row-1) Input images. (Row-2) Mesh recovery by a template-based strategy. (Row-3) Structure prediction by a template-free strategy. (Row-4) Structure prediction by our sketcher w/o/ bare structure prior. (Row-5) Structure disentanglement by our full sketcher. Red circles indicate the artifacts in the results.
                </div>
                
                <div style="text-align:center"><img class="lazy" width="90%" src="./compareCyclegan.jpg"></div>
                <div style="font-size:0.75em;text-align:center;line-height:1.4em;">
                    <b>Comparisons on learning paradigms.</b> From left to right: The input, <a target="_blank" href="https://junyanz.github.io/CycleGAN/" style="font-weight:bold">CycleGAN</a>, <a target="_blank" href="https://taesung.me/ContrastiveUnpairedTranslation/" style="font-weight:bold">CUT</a>, <a target="_blank" href="https://phillipi.github.io/pix2pix/" style="font-weight:bold">Pix2pix</a>, and Ours. 
                </div>
                
                <div style="text-align:center"><img class="lazy" width="90%" src="./compareNST.jpg"></div>
                <div style="font-size:0.75em;text-align:center;line-height:1.4em;">
                    <b>Comparisons to the SOTA NSTs.</b> From left to right: The input, <a target="_blank" href="https://github.com/facebookresearch/pytorch3d" style="font-weight:bold">Diff-Render</a>, <a target="_blank" href="https://github.com/clovaai/WCT2" style="font-weight:bold">WCT2</a>, <a target="_blank" href="https://github.com/nkolkin13/STROTSS" style="font-weight:bold">STROTSS</a>, and Ours.
                </div>
                
                <!-- Reference -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:1em">Reference</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="background-color:#ddd;margin-bottom:1em">
                    <div style="text-align:left;font-size:0.75em;line-height:1.4em;padding:1em 1em">Zimeng Zhao, Binghui Zuo, Zhiyu Long and Yangang Wang. "<strong>Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination</strong>". <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition, (CVPR)</i>, 2023.</div>
                </div>

                <!-- acknowledgements -->
                <div style="font-size:0.75em;text-align:left;line-height:1.4em"><strong>Acknowledgments:</strong> This work was supported in part by the National Natural Science Foundation of China (No. 62076061), in part by the Natural Science Foundation of Jiangsu Province (No. BK20220127).</div>
            </article>
        </div>

		<!-- Footer -->
		<div id="footer">
            <ul style="list-style: none;"> 
                <li>&copy; 2019 Dr. <a href="../index.html" style="color:#EB7500;text-decoration:none;">Yangang Wang</a>. All Rights Reserved. </li>
                <li><div style="display: flex;justify-content: center;align-items: center;"><img src="../../images/beian.png">&nbsp;<a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32011502011122" style="color:rgba(255, 255, 255, 0.45);text-decoration:none;">苏公网安备 32011502011122号</a>&nbsp;&nbsp;&nbsp;<a target="_blank" href="http://www.beian.miit.gov.cn/" style="color:rgba(255, 255, 255, 0.45);text-decoration:none;">京ICP备20006779号</a></div></li>
            </ul>
		</div>
    </div>

    <script src="../../assets/js/main.js"></script>
    <script>
		$(function() {
			$('.lazy').Lazy({
                effect : "fadeIn"
            });
		});
	</script>
    <!-- <script>setTimeout(function(){document.getElementById('outdoorvideo').src = 'https://www.youtube.com/embed/Fa_y9rxMPPg';},50)</script> -->

</body></html>