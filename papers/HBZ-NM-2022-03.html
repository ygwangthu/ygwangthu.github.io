<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <!--meta http-equiv="Content-Type" content="text/html; charset=UTF-8"-->
	<title>Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture</title>
	
	<meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1">
    <!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="icon" href="../images/logo.ico">
    <link rel="stylesheet" href="../assets/css/main.css">
    
    <!-- Scripts -->
    <script src="../assets/js/jquery.min.js"></script>
    <script src="../assets/js/jquery.lazy.min.js"></script>
</head>

<body class="">
    <div id="wrapper">
        <div id="main" class="panel" style="margin-top:2em;line-height:1.3em">
            <article id="news" class="panel">
                <!-- head -->
                <div style="font-size:0.75em;text-align:center;padding-top:0.5em">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</div>
                <div style="font-size:1.0em;font-weight:bold;padding:1em 0.5em;text-align:center">Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture</div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;">Buzhen Huang, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;">Liang Pan, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;">Yuan Yang, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;">Jingyi Ju, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="https://www.yangangwang.com/" style="color:#EB7500">Yangang Wang</a></div>              
                </div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;"><a target="_blank" href="http://www.seu.edu.cn/english/main.htm" style="font-weight:bold">Southeast University</a></div>
                </div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;;margin-bottom:1em">

                <!-- teaser -->
                <div style="text-align:center;"><img class="lazy" width="100%" src="./NeuralMocon/HUANG-Neural_MoCon-2022-03-pipeline.png" style=""></div>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em"><div style="font-weight:bold;display:inline-block">Overview of the proposed two-branch structure.</div> Our method first estimates reference motion with accurate human-scene interaction as well as human shape from a monocular RGB video (a). Then, a prior regresses a distribution from the state of physical character and the reference pose to sample target poses (b). The physics simulator is used to obtain a physically plausible pose for each sample (c). The sample with the lowest loss is adopted and used for the next frame after sample evaluation (d). </div>

                <!-- abstract -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em;">Abstract</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="text-align:left;font-size:0.75em;line-height:1.4em">Due to the visual ambiguity, purely kinematic formulations on monocular human motion capture are often physically incorrect, biomechanically implausible, and can not reconstruct accurate interactions. In this work, we focus on exploiting the high-precision and non-differentiable physics simulator to incorporate dynamical constraints in motion capture. Our key-idea is to use real physical supervisions to train a target pose distribution prior for sampling-based motion control to capture physically plausible human motion. To obtain accurate reference motion with terrain interactions for the sampling, we first introduce an interaction constraint based on SDF (Signed Distance Field) to enforce appropriate ground contact modeling. We then design a novel two-branch decoder to avoid stochastic error from pseudo ground-truth and train a distribution prior with the non-differentiable physics simulator. Finally, we regress the sampling distribution from the current state of the physical character with the trained prior and sample satisfied target poses to track the estimated reference motion. Qualitative and quantitative results show that we can obtain physically plausible human motion with complex terrain interactions, human shape variations, and diverse behaviors. </div>

                <!-- Code -->
                <!-- <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Running Code and Trained Model</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="font-size:0.75em;text-align:left;line-height:1.4em;"> -->
                    <!--cuda、cudnn、GPU版本均未修改-->
                    <!-- Here, we release the source code (under Nvidia 1080Ti with <a target="_blank" href="https://developer.nvidia.com/cuda-downloads">CUDA10.1</a> and <a target="_blank" href="https://developer.nvidia.com/cudnn">cuDNN7.6</a>, Win10). You can directly run the code with the given model. <div style="font-weight:bold;display:inline-block"></div>
                    <p> -->
                        <!--地址填空-->
                        <!-- <span style="font-size:0.9em;"><a target="_blank" href="  ">Source Code <i class="fa fa-arrow-right"></i></a></span>&nbsp;&nbsp;&nbsp;&nbsp;<span style="font-size:0.9em;"><a target="_blank" href="  ">Trained Model<i class="fa fa-arrow-right"></i></a></span>
                    </p>
                    The rights to copy, distribute, and use the code are being given access to are under the control of Yangang Wang, director of the Vision and Cognition Lab, Southeast University. In this case, credit must be given to: *Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture*. <div style="font-weight:bold;display:inline-block">Any commercial use is not allowed</div>. I am very glad to receive your feedbacks about this code.  -->
<!-- 
                </div> -->
               

                <!-- results -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Results</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="text-align:center"><img class="lazy" width="90%" src="./NeuralMocon/HUANG-Neural_MoCon-2022-03-results.png"></div>
                <div style="font-size:0.75em;text-align:center;line-height:1.4em;">
                    Results on Human3.6M (row 1-2), 3DOH (row 3-4) and GPA (row 5-6) dataset.
                </div>

                <!-- materials -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Materials</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <table>
                    <tbody><tr>
                        <td style="width:25%;padding:0em 0.5em;border-right:1px solid #dedddd">
                            <a target="_blank" title="paper"><div class="image_carousel"><img class="lazy" width="100%" src="./NeuralMocon/HUANG-Neural_MoCon-2022-03-papers.png" style=""></div></a>
                            <hr style="height:1px;border:none;border-top:1px solid #dedddd;">
                            <div style="font-size:0.7em;text-align:left;line-height:1.4em">Related links</div>
                            <ul style="font-size:0.6em;line-height:1.4em;margin-top:0em;">
                                <li><a target="_blank" href="HBZ-NM-2022-03.pdf">Download Paper</a></li>
                                <li><a target="_blank" href="HBZ-NM-2022-03-supp.pdf">Download Supplementary Material</a></li>
                                <li><a target="_blank" href="HBZ-NM-2022-03.bib">Download Bibtex</a></li>
                                <li><a target="_blank">IEEE Digital Library</a></li>
                                <li><a target="_blank" href="https://www.bilibili.com/video/BV1W94y1f7ht">Video on bilibili</a></li>
                            </ul>
                        </td>
                        <td style="width:70%;padding:0em 0.3em">
                            <div class="video-container">
                                <iframe id="neuralmocon" src="https://player.bilibili.com/player.html?aid=340115056&bvid=BV1W94y1f7ht&cid=559167834&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
                            </div>
                        </td>
                    </tr>
                </tbody></table>

                <!-- Reference -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:1em">Reference</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em">
                <div style="background-color:#ddd;margin-bottom:1em">
                    <div style="text-align:left;font-size:0.75em;line-height:1.4em;padding:1em 1em">Buzhen Huang, Liang Pan, Yuan Yang, Jingyi Ju and Yangang Wang. "<strong>Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture</strong>". <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition, (CVPR)</i>, 2022.</div>
                </div>

                <!-- acknowledgements -->
                <div style="font-size:0.75em;text-align:left;line-height:1.4em"><strong>Acknowledgments:</strong> This work was supported in part by the National Key R&D Program of China under Grant 2018YFB1403900, the National Natural Science Foundation of China (No. 62076061), the ``Young Elite Scientists Sponsorship Program by CAST" (No. YES20200025), and the ``Zhishan Young Scholar" Program of Southeast University (No. 2242021R41083). </div>
            </article>
        </div>

        <!-- Footer -->
		<div id="footer">
            <ul style="list-style: none;"> 
                <li>© 2019 Dr. <a href="https://www.yangangwang.com/index.html" style="color:#EB7500;text-decoration:none;">Yangang Wang</a>. All Rights Reserved. </li>
                <li><div style="display: flex;justify-content: center;align-items: center;"><img src="../images/beian.png">&nbsp;<a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32011502011122" style="color:rgba(255, 255, 255, 0.45);text-decoration:none;">苏公网安备 32011502011122号</a>&nbsp;&nbsp;&nbsp;<a target="_blank" href="https://beian.miit.gov.cn/" style="color:rgba(255, 255, 255, 0.45);text-decoration:none;">京ICP备20006779号</a></div></li>
            </ul>
		</div>
    </div>

    <script src="../assets/js/main.js"></script>
    <script>
		$(function() {
			$('.lazy').Lazy({
                effect : "fadeIn"
            });
		});
	</script>
    <script>setTimeout(function(){document.getElementById('neuralmocon').src = 'https://player.bilibili.com/player.html?aid=340115056&bvid=BV1W94y1f7ht&cid=559167834&page=1';},50)</script>
</body></html>