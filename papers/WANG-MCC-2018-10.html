<!DOCTYPE HTML>
<html>
<head>
	<title>Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Images</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1" />
    <!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="icon" href="../images/logo.ico" />
    <link rel="stylesheet" href="../assets/css/main.css" />
    
    <!-- Scripts -->
    <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdn.bootcss.com/jquery.lazy/1.7.10/jquery.lazy.min.js"></script>
</head>

<body>
    <div id="wrapper">
        <div id="main" class="panel shadow" style="margin-top:2em;line-height:1.3em">
            <article id="news" class="panel">
                <!-- head -->
                <div style="font-size:0.75em;text-align:center;padding-top:0.5em">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</div>
                <div style="font-size:1.0em;font-weight:bold;padding:1em 0.5em;text-align:center">Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Images</div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;width:8em;"><a target="_blank" href="http://www.yangangwang.com/" style="color:#EB7500">Yangang Wang</a><sup>1</sup>, </div>
                    <div style="display:inline-block;width:6em;">Cong Peng<sup>2</sup>, </div>
                    <div style="display:inline-block;width:6em;"><a target="_blank" href="http://www.liuyebin.com/" style="color:#EB7500">Yebin Liu</a><sup>3</sup></div>                    
                </div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;width:12em"><sup>1</sup><a target="_blank" href="http://www.seu.edu.cn/english/main.htm" style="font-weight:bold">Southeast University</a></div>
                    <div style="display:inline-block;width:26em"><sup>2</sup><a target="_blank" href="http://www.nuaa.edu.cn/" style="font-weight:bold">Nanjing University of Aeronautics and Astronautics</a></div>
                    <div style="display:inline-block;width:12em"><sup>3</sup><a target="_blank" href="http://www.tsinghua.edu.cn/publish/newthuen/index.html" style="font-weight:bold">Tsinghua University</a></div>
                    
                </div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;;margin-bottom:1em"/>

                <!-- teaser -->
                <div style="text-align:center;"><img class="lazy" width="779" height="224" data-src="WANG-MCC-2018-10-teaser.png"></div>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em"><div style="font-weight:bold;display:inline-block">Network architecture.</div> The proposed network structure includes two stages: mask prediction stage and pose prediction stage. See the original paper for more details.</div>

                <!-- abstract -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em;">Abstract</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em">We present a cascaded convolutional neural network for 2D hand pose estimation from single in-the-wild RGB images. Inspired by the commonly used silhouette information in the generative pose estimation approaches, we build the cascaded network with two stages, including mask prediction stage as well as pose estimation stage. We find that the two stages network architecture for end-to-end training could benefit with each other for detecting the hand mask and 2D pose. To further improve the hand pose detection accuracy, we contribute a new RGB hand dataset named <span style="font-weight:bold">OneHand10K</span>, which contains 10K RGB images. Each image contains one single hand. We manually obtained the segmented mask and labeled keypoints for guided learning. We hope that this dataset will give a benchmark and encourage more people to perform research on this challenging topic. Experiments on the validation dataset have demonstrated the superior performance of the proposed cascaded convolutional neural network.</div>

                <!-- Code -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Training Code</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="font-size:0.75em;text-align:left;line-height:1.4em;">
                    Our network was originally trained by <a href="http://caffe.berkeleyvision.org/">Caffe</a>. The training codes are listed below:
                    <p>
                        <span style="font-size:0.9em;"><a target="_blank" href="WANG-MCC-2018-10-code/train_val_total.prototxt">Training Network <i class="fa fa-arrow-right"></i></a></span>&nbsp&nbsp&nbsp&nbsp
                        <span style="font-size:0.9em;"><a target="_blank" href="WANG-MCC-2018-10-code/solver_total.prototxt">Solver <i class="fa fa-arrow-right"></i></a></span>&nbsp&nbsp&nbsp&nbsp
                        <span style="font-size:0.9em;"><a target="_blank" href="WANG-MCC-2018-10-code/pose_deploy_total.prototxt">Deploy Network <i class="fa fa-arrow-right"></i></a></span>
                    </p>
                    <span>Note that we have updated our training platform to <a href="https://github.com/pytorch/pytorch/tree/master/caffe2">Caffe2</a>, the original trained weights are not maintained anymore. You can re-train the network and reproduce the results of this paper by yourself</span>.<br><br>

                    <span style="color:#FF1A1A;font-weight:bold">Update on 2019-07</span><br>
                    <span style="font-weight:bold"> We have released a more accurate and faster method named as <a href="WANG-SRH-2019-07.html">SRHandNet</a> for real-time 2D hand pose estimation. The code is available and please check the new method <i class="fa fa-smile-o"></i>.</span>
                </div>

                <!-- Dataset -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">OneHand10K Dataset</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="font-size:0.75em;text-align:left;line-height:1.4em;">
                    To request the dataset, please send an <a href="mailto:&#121;&#103;&#119;&#97;&#110;g&#116;&#104;&#117;&#64;g&#109;a&#105;&#108;.&#99;&#111;m?subject=Request%20the%20OneHand10K%20Dataset">email</a> stating
                    <ol>
                        <li>
                            your name, title or position, and institution or affiliation. (<span style="font-weight:bold">NOTE</span>: this dataset is <span style="font-weight:bold">ONLY</span> for research and non-commercial use. For copyright issue, your institution or affiliation is a must and we do not accept the requirement from individual researchers or students. If you are a student, we encourage you to ask your advisor or the faculty from your research institute, college or university to request the dataset. <span style="color:#FF1A1A;font-weight:bold">Institution E-mail address is required.</span>)
                        </li>
                        <li>
                            a statement saying that you accept the following terms of licensing (<span style="font-weight:bold">please copy the licensing text into your email</span>):
                            <p style="font-weight:bold">The rights to copy, distribute, and use the OneHand10K dataset (henceforth called "OneHand10K") you are being given access to are under the control of Yangang Wang, director of the Vision and Cognition Lab, Southeast University. You are hereby given permission to copy this data in electronic or hardcopy form for your own scientific use and to distribute it for scientific use to colleagues within your research group. Inclusion of images or video made from this data in a scholarly publication (printed or electronic) is also permitted. In this case, credit must be given to the publication: *Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Image*. For any other use, including distribution outside your research group, written permission is required from Yangang Wang. Any commercial use is not allowed. Commercial use includes but is not limited to sale of the data, derivatives, replicas, images, or video, inclusion in a product for sale, or inclusion in advertisements (printed or electronic), on commercially-oriented web sites, or in trade shows.</p>
                        </li>
                    </ol>
                </div>

                <!-- results -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Results</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="text-align:center"><img class="lazy" width="779" height="514" data-src="WANG-MCC-2018-10-result.png"></div>

                <!-- materials -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Materials</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <table>
                    <tr>
                        <td style="width:25%;padding:0em 0.5em;border-right:1px solid #dedddd">
                            <a target="_blank" href="WANG-MCC-2018-10.pdf" title="paper"><div class="image_carousel"><img class="lazy" width="183" height="237" data-src="WANG-MCC-2018-10-paper.png"></div></a>
                            <hr style="height:1px;border:none;border-top:1px solid #dedddd;" />
                            <div style="font-size:0.7em;text-align:left;line-height:1.4em">Related links</div>
                            <ul style="font-size:0.6em;line-height:1.4em;margin-top:0em;">
                                <li><a target="_blank" href="WANG-MCC-2018-10.pdf">Download Paper</a></li>
                                <li><a target="_blank" href="WANG-MCC-2018-10.bib">Download Bibtex</a></li>
                                <li><a target="_blank" href="https://doi.org/10.1109/TCSVT.2018.2879980">IEEE Digital Library</a></li>
                                <li><a target="_blank" href="https://youtu.be/L4u7p3p5IDE">Video on YouTube</a></li>
                            </ul>
                        </td>
                        <td style="width:75%;padding:0em 0.3em">
                            <div class="video-container">
                                <iframe id="hand2d" src="" width="1280" height="720" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </td>
                    </tr>
                </table>

                <!-- Reference -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:1em">Reference</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="background-color:#ddd;margin-bottom:1em">
                    <div style="text-align:left;font-size:0.75em;line-height:1.4em;padding:1em 1em">Yangang Wang, Cong Peng and Yebin Liu. "<strong>Mask-pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Images</strong>". <i>IEEE Transactions on Circuits and Systems for Video Technology</i>, 29(11), 3258 - 3268, 2019.</div>
                </div>

                <!-- acknowledgements -->
                <div style="font-size:0.75em;text-align:left;line-height:1.4em"><strong>Acknowledgments:</strong> The authors would like to thank Xiaoquan Lv, Biyao Shao, Junjie Zhu and Yining Xie to help us to build the dataset. This work was supported by the National Natural Science Foundation of China (No. 61806054, 6170320), Natural Science Foundation of Jiangsu Province (No. BK20180355 and BK20170812) and Foundation of Southeast University (No. 3208008410 and 1108007121).</div>
            </article>
        </div>

        <!-- Footer -->
		<div id="footer">
            <ul style="list-style: none;"> 
                <li>&copy; 2019 Dr. <a href="../index.html" style="color:#EB7500;text-decoration:none;">Yangang Wang</a>. All Rights Reserved. </li>
                <li><a target="_blank" href="http://www.beian.miit.gov.cn/" style="color:#EB7500;text-decoration:none;">京ICP备20006779号</a></li>
            </ul>
		</div>
    </div>

    <script src="../assets/js/main.js"></script>
    <script>
		$(function() {
			$('.lazy').Lazy({
                effect : "fadeIn"
            });
		});
	</script>
    <script>setTimeout(function(){document.getElementById('hand2d').src = 'https://www.youtube.com/embed/L4u7p3p5IDE';},50)</script>
</body>
</html>