<!DOCTYPE HTML>
<html>
<head>
	<title>Outdoor Markerless Motion Capture with Sparse Handheld Video Cameras</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1" />
    <!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="icon" href="../images/logo.ico" />
    <link rel="stylesheet" href="../assets/css/main.css" />
    
    <!-- Scripts -->
    <script src="../assets/js/jquery.min.js"></script>
    <script src="../assets/js/jquery.lazy.min.js"></script>
</head>

<body class="is-preload">
    <div id="wrapper">
        <div id="main" class="panel" style="margin-top:2em;line-height:1.3em">
            <article id="news" class="panel">
                <!-- head -->
                <div style="font-size:0.75em;text-align:center;padding-top:0.5em">IEEE Transactions on Visualization and Computer Graphics (TVCG)</div>
                <div style="font-size:1.0em;font-weight:bold;padding:1em 0.5em;text-align:center">Outdoor Markerless Motion Capture with Sparse Handheld Video Cameras</div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;"><a target="_blank" href="http://yangangwang.com/" style="color:#EB7500">Yangang Wang</a><sup>1</sup>, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="http://www.liuyebin.com/" style="color:#EB7500">Yebin Liu</a><sup>2</sup>, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="https://www.microsoft.com/en-us/research/people/xtong/" style="color:#EB7500">Xin Tong</a><sup>1</sup>, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="http://media.au.tsinghua.edu.cn/qhdai.html" style="color:#EB7500">Qionghai Dai</a><sup>2</sup>, </div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><a target="_blank" href="http://www.cs.sfu.ca/~pingtan/" style="color:#EB7500">Ping Tan</a><sup>3</sup></div>
                </div>
                <div style="font-size:0.75em;padding-bottom:1em;text-align:center">
                    <div style="display:inline-block;"><sup>1</sup><a target="_blank" href="http://research.microsoft.com/" style="font-weight:bold">Microsoft Research</a></div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><sup>2</sup><a target="_blank" href="http://www.tsinghua.edu.cn/publish/newthuen/index.html" style="font-weight:bold">Tsinghua University</a></div>&nbsp;&nbsp;
                    <div style="display:inline-block;"><sup>3</sup><a target="_blank" href="https://www.sfu.ca/" style="font-weight:bold">Simon Fraser University</a></div>
                </div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;;margin-bottom:1em"/>

                <!-- teaser -->
                <div style="text-align:center;"><img class="lazy" width="100%" data-src="WANG-OMM-2017-04-teaser.png"></div>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em"><div style="font-weight:bold;display:inline-block">System pipeline.</div> (a) Our system takes multiple synchronized videos captured by handheld cameras as input. (b) We adopt the <a target="_blank" href="http://drone.sjtu.edu.cn/dpzou/project/coslam.php?video_switch=false">CoSLAM</a> method to calibrate all cameras in 3D space. (c) Our motion tracking system estimates the skeleton motion with a sample-based method. (d) We use a gradient-based method to refine the skeleton pose.</div>

                 <!-- abstract -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em;">Abstract</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="text-align:left;font-size:0.75em;line-height:1.4em">We present a method for outdoor markerless motion capture with sparse handheld video cameras. In the simplest setting, it only involves two mobile phone cameras following the character. This setup can maximize the flexibilities of data capture and broaden the applications of motion capture. To solve the character pose under such challenge settings, we exploit the generative motion capture methods and propose a novel model-view consistency that considers both foreground and background in the tracking stage. The background is modeled as a deformable 2D grid, which allows us to compute the background-view consistency for sparse moving cameras. The 3D character pose is tracked with a global-local optimization through minimizing our consistency cost. A novel L1 motion regularizer is also proposed in the optimization to constrain the solution pose space. The whole process of the proposed method is simple as frame by frame video segmentation is not required. Our method outperforms several alternative methods on various examples demonstrated in the paper.</div>

                <!-- results -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Results</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="text-align:center"><img class="lazy" width="100%" data-src="WANG-OMM-2017-04-result.png"></div>

                <!-- materials -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Materials</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <table>
                    <tr>
                        <td style="width:25%;padding:0em 0.5em;border-right:1px solid #dedddd">
                            <a target="_blank" href="WANG-OMM-2017-04.pdf" title="paper"><div class="image_carousel"><img class="lazy" width="100%" data-src="WANG-OMM-2017-04-paper.png"></div></a>
                            <hr style="height:1px;border:none;border-top:1px solid #dedddd;" />
                            <div style="font-size:0.7em;text-align:left;line-height:1.4em">Related links</div>
                            <ul style="font-size:0.6em;line-height:1.4em;margin-top:0em;">
                                <li><a target="_blank" href="WANG-OMM-2017-04.pdf">Download Paper</a></li>
                                <li><a target="_blank" href="WANG-OMM-2017-04.bib">Download Bibtex</a></li>
                                <li><a target="_blank" href="https://doi.org/10.1109/TVCG.2017.2693151">IEEE Digital Library</a></li>
                                <li><a target="_blank" href="https://youtu.be/Fa_y9rxMPPg">Video on YouTube</a></li>
                            </ul>
                        </td>
                        <td style="width:75%;padding:0em 0.3em">
                            <div class="video-container">
                                <iframe id="outdoorvideo" src="" width="1280" height="720" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </td>
                    </tr>
                </table>

                <!-- Related Works -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:2em">Related Work</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="font-size:0.75em;text-align:left;line-height:1.4em;">
                    Outdoor markerless motion capture has the potential to enormously broaden the diversity and richness of motions that can be captured. Check out some related work below:
                    <ul style="font-size:0.95em;line-height:1.4em;padding-left:2.5em;margin-top:0.7em;">
                        <li style="padding-bottom:0.5em"><a target="_blank" href="https://www.tnt.uni-hannover.de/papers/data/767/767_1.pdf">Markerless Motion Capture with Unsynchronized Moving Cameras</a><br />By Hasler et al. (CVPR2009)</li>
                        <li style="padding-bottom:0.5em"><a target="_blank" href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/141/Pons-Moll_ICCV2011.pdf">Outdoor Human Motion Capture using Inverse Kinematics and von Mises-Fisher Sampling</a><br />By Pons-Moll et al. (ICCV2011)</li>
                        <li style="padding-bottom:0.5em"><a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Elhayek_Efficient_ConvNet-Based_Marker-Less_2015_CVPR_paper.pdf">Efficient ConvNet-based Marker-less Motion Capture in General Scenes with a Low Number of Cameras</a><br />By Elhayek et al. (CVPR2015)</li>
                        <li style="padding-bottom:0.5em"><a target="_blank" href="http://gvv.mpi-inf.mpg.de/projects/OutdoorPerfcap/">Model-based Outdoor Performance Capture</a><br />By Robertini et al. (3DV2016)</li>
					</ul> 
                </div>

                <!-- Reference -->
                <div style="font-size:0.9em;font-weight:bold;padding-top:1em">Reference</div>
                <hr style="height:1px;border:none;border-top:1px solid #dedddd;margin-bottom:1em"/>
                <div style="background-color:#ddd;margin-bottom:1em">
                    <div style="text-align:left;font-size:0.75em;line-height:1.4em;padding:1em 1em">Yangang Wang, Yebin Liu, Xin Tong, Qionghai Dai and Ping Tan. "<strong>Outdoor Markerless Motion Capture with Sparse Handheld Video Cameras</strong>". <i>IEEE Transactions on Visualization and Computer Graphics</i>, 24(5):1856-1866, 2018.</div>
                </div>

                <!-- acknowledgements -->
                <div style="font-size:0.75em;text-align:left;line-height:1.4em"><strong>Acknowledgments:</strong> We would like thank <a target="_blank" href="http://www.linkaimo.org/">Kaimo Lin</a> to help us to capture the majority of data. This work was supported in part by the National Key Foundation for Exploring Scientific Instrument No.2013YQ140517, the National Science Foundation of China (NSFC) No.61522111 and No.61531014.</div>
            </article>
        </div>

        <!-- Footer -->
		<div id="footer">
            <ul style="list-style: none;"> 
                <li>&copy; 2019 Dr. <a href="../index.html" style="color:#EB7500;text-decoration:none;">Yangang Wang</a>. All Rights Reserved. </li>
                <li><div style="display: flex;justify-content: center;align-items: center;"><img src="../images/beian.png">&nbsp;<a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32011502011122" style="color:rgba(255, 255, 255, 0.45);text-decoration:none;">苏公网安备 32011502011122号</a>&nbsp;&nbsp;&nbsp;<a target="_blank" href="http://www.beian.miit.gov.cn/" style="color:rgba(255, 255, 255, 0.45);text-decoration:none;">京ICP备20006779号</a></div></li>
            </ul>
		</div>
    </div>

    <script src="../assets/js/main.js"></script>
    <script>
		$(function() {
			$('.lazy').Lazy({
                effect : "fadeIn"
            });
		});
	</script>
    <script>setTimeout(function(){document.getElementById('outdoorvideo').src = 'https://www.youtube.com/embed/Fa_y9rxMPPg';},50)</script>
</body>
</html>